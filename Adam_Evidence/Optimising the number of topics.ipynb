{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75695ac3",
   "metadata": {},
   "source": [
    "# Optimising the number of topics\n",
    "\n",
    "We needed to find an optimal number of topics that we can use across all our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d43881ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import ldaseqmodel\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import jaccard_score\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf28599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Status</th>\n",
       "      <th>Description</th>\n",
       "      <th>References</th>\n",
       "      <th>Phase</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CVE-1999-0001</td>\n",
       "      <td>Candidate</td>\n",
       "      <td>['ipinputc', 'bsdderived', 'tcpip', 'implement...</td>\n",
       "      <td>BUGTRAQ:19981223 Re: CERT Advisory CA-98.13 - ...</td>\n",
       "      <td>Modified (20051217)</td>\n",
       "      <td>MODIFY(1) Frech  |     NOOP(2) Northcutt, W...</td>\n",
       "      <td>Christey&gt; A Bugtraq posting indicates that the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CVE-1999-0002</td>\n",
       "      <td>Entry</td>\n",
       "      <td>['buffer', 'overflow', 'nfs', 'mountd', 'give'...</td>\n",
       "      <td>BID:121   |   URL:http://www.securityfocus.com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CVE-1999-0003</td>\n",
       "      <td>Entry</td>\n",
       "      <td>['execute', 'command', 'root', 'buffer', 'over...</td>\n",
       "      <td>BID:122   |   URL:http://www.securityfocus.com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CVE-1999-0004</td>\n",
       "      <td>Candidate</td>\n",
       "      <td>['mime', 'buffer', 'overflow', 'email', 'clien...</td>\n",
       "      <td>CERT:CA-98.10.mime_buffer_overflows   |   MS:M...</td>\n",
       "      <td>Modified (19990621)</td>\n",
       "      <td>ACCEPT(8) Baker, Cole, Collins, Dik, Landfi...</td>\n",
       "      <td>Frech&gt; Extremely minor, but I believe e-mail i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CVE-1999-0005</td>\n",
       "      <td>Entry</td>\n",
       "      <td>['arbitrary', 'command', 'execution', 'imap', ...</td>\n",
       "      <td>BID:130   |   URL:http://www.securityfocus.com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166896</th>\n",
       "      <td>CVE-2021-46482</td>\n",
       "      <td>Candidate</td>\n",
       "      <td>['jsish', 'v', 'discover', 'contain', 'heap', ...</td>\n",
       "      <td>MISC:https://github.com/pcmacdon/jsish/issues/66</td>\n",
       "      <td>Assigned (20220124)</td>\n",
       "      <td>None (candidate not yet proposed)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166897</th>\n",
       "      <td>CVE-2021-46483</td>\n",
       "      <td>Candidate</td>\n",
       "      <td>['jsish', 'v', 'discover', 'contain', 'heap', ...</td>\n",
       "      <td>MISC:https://github.com/pcmacdon/jsish/issues/62</td>\n",
       "      <td>Assigned (20220124)</td>\n",
       "      <td>None (candidate not yet proposed)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166898</th>\n",
       "      <td>CVE-2021-46559</td>\n",
       "      <td>Candidate</td>\n",
       "      <td>['firmware', 'moxa', 'tn', 'device', 'weak', '...</td>\n",
       "      <td>MISC:https://www.moxa.com/en/support/product-s...</td>\n",
       "      <td>Assigned (20220126)</td>\n",
       "      <td>None (candidate not yet proposed)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166899</th>\n",
       "      <td>CVE-2021-46560</td>\n",
       "      <td>Candidate</td>\n",
       "      <td>['firmware', 'moxa', 'tn', 'device', 'allow', ...</td>\n",
       "      <td>MISC:https://www.moxa.com/en/support/product-s...</td>\n",
       "      <td>Assigned (20220126)</td>\n",
       "      <td>None (candidate not yet proposed)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166900</th>\n",
       "      <td>CVE-2021-46561</td>\n",
       "      <td>Candidate</td>\n",
       "      <td>['controllerorgcontrollerorgcontrollerjs', 'cv...</td>\n",
       "      <td>CONFIRM:https://github.com/CVEProject/cve-serv...</td>\n",
       "      <td>Assigned (20220126)</td>\n",
       "      <td>None (candidate not yet proposed)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166901 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name     Status  \\\n",
       "0        CVE-1999-0001  Candidate   \n",
       "1        CVE-1999-0002      Entry   \n",
       "2        CVE-1999-0003      Entry   \n",
       "3        CVE-1999-0004  Candidate   \n",
       "4        CVE-1999-0005      Entry   \n",
       "...                ...        ...   \n",
       "166896  CVE-2021-46482  Candidate   \n",
       "166897  CVE-2021-46483  Candidate   \n",
       "166898  CVE-2021-46559  Candidate   \n",
       "166899  CVE-2021-46560  Candidate   \n",
       "166900  CVE-2021-46561  Candidate   \n",
       "\n",
       "                                              Description  \\\n",
       "0       ['ipinputc', 'bsdderived', 'tcpip', 'implement...   \n",
       "1       ['buffer', 'overflow', 'nfs', 'mountd', 'give'...   \n",
       "2       ['execute', 'command', 'root', 'buffer', 'over...   \n",
       "3       ['mime', 'buffer', 'overflow', 'email', 'clien...   \n",
       "4       ['arbitrary', 'command', 'execution', 'imap', ...   \n",
       "...                                                   ...   \n",
       "166896  ['jsish', 'v', 'discover', 'contain', 'heap', ...   \n",
       "166897  ['jsish', 'v', 'discover', 'contain', 'heap', ...   \n",
       "166898  ['firmware', 'moxa', 'tn', 'device', 'weak', '...   \n",
       "166899  ['firmware', 'moxa', 'tn', 'device', 'allow', ...   \n",
       "166900  ['controllerorgcontrollerorgcontrollerjs', 'cv...   \n",
       "\n",
       "                                               References  \\\n",
       "0       BUGTRAQ:19981223 Re: CERT Advisory CA-98.13 - ...   \n",
       "1       BID:121   |   URL:http://www.securityfocus.com...   \n",
       "2       BID:122   |   URL:http://www.securityfocus.com...   \n",
       "3       CERT:CA-98.10.mime_buffer_overflows   |   MS:M...   \n",
       "4       BID:130   |   URL:http://www.securityfocus.com...   \n",
       "...                                                   ...   \n",
       "166896   MISC:https://github.com/pcmacdon/jsish/issues/66   \n",
       "166897   MISC:https://github.com/pcmacdon/jsish/issues/62   \n",
       "166898  MISC:https://www.moxa.com/en/support/product-s...   \n",
       "166899  MISC:https://www.moxa.com/en/support/product-s...   \n",
       "166900  CONFIRM:https://github.com/CVEProject/cve-serv...   \n",
       "\n",
       "                      Phase  \\\n",
       "0       Modified (20051217)   \n",
       "1                       NaN   \n",
       "2                       NaN   \n",
       "3       Modified (19990621)   \n",
       "4                       NaN   \n",
       "...                     ...   \n",
       "166896  Assigned (20220124)   \n",
       "166897  Assigned (20220124)   \n",
       "166898  Assigned (20220126)   \n",
       "166899  Assigned (20220126)   \n",
       "166900  Assigned (20220126)   \n",
       "\n",
       "                                                    Votes  \\\n",
       "0          MODIFY(1) Frech  |     NOOP(2) Northcutt, W...   \n",
       "1                                                     NaN   \n",
       "2                                                     NaN   \n",
       "3          ACCEPT(8) Baker, Cole, Collins, Dik, Landfi...   \n",
       "4                                                     NaN   \n",
       "...                                                   ...   \n",
       "166896                  None (candidate not yet proposed)   \n",
       "166897                  None (candidate not yet proposed)   \n",
       "166898                  None (candidate not yet proposed)   \n",
       "166899                  None (candidate not yet proposed)   \n",
       "166900                  None (candidate not yet proposed)   \n",
       "\n",
       "                                                 Comments  \n",
       "0       Christey> A Bugtraq posting indicates that the...  \n",
       "1                                                     NaN  \n",
       "2                                                     NaN  \n",
       "3       Frech> Extremely minor, but I believe e-mail i...  \n",
       "4                                                     NaN  \n",
       "...                                                   ...  \n",
       "166896                                                NaN  \n",
       "166897                                                NaN  \n",
       "166898                                                NaN  \n",
       "166899                                                NaN  \n",
       "166900                                                NaN  \n",
       "\n",
       "[166901 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/formatted_df.csv\").drop(columns = ['Unnamed: 0'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850af1b9",
   "metadata": {},
   "source": [
    "We found that the way that we saved the data frame, meant that the Description column was read as a string rather than a list as it was intended. Therefore, we had to apply the function literal_eval which allows us to convert the string of a stored list into a python list. We then separate the description column into a list to allow easier access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349be42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description'] = df['Description'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c97422",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = df['Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6dfaf",
   "metadata": {},
   "source": [
    "To separate the data into years easily we find the boundary indices for each year. We will use that the year of the vulnerability is the 4-8 characters of each of the CVE names. Therefore, we will extract these and count the instances for each one and add it to the last boundary index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0933fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1541, 2778, 4313, 6663, 8161, 10794, 15380, 22238, 28578, 35549, 40436, 45428, 50015, 55416, 61536, 69815, 77731, 86931, 101250, 116731, 132000, 149784, 166901]\n"
     ]
    }
   ],
   "source": [
    "names = df['Name']\n",
    "year = []\n",
    "for instance in names:\n",
    "    year.append(int(instance[4:8]))\n",
    "year_count = [0]\n",
    "for i in range(23):\n",
    "    if i == 0:\n",
    "        year_count.append(year.count(i+1999))\n",
    "    else:\n",
    "        year_count.append(year.count(i+1999) + year_count[i]) \n",
    "print(year_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd10464",
   "metadata": {},
   "source": [
    "Here we create a dictionary of words that occur in the whole data set, allowing us to index each of these words. We also format the documents of each year into a matrix which indicates how many times each word occurs in each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36bd80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = gensim.corpora.Dictionary(desc)\n",
    "doc_word_matrix_array = []\n",
    "for i in range(23):\n",
    "    doc_word_matrix_array.append([vocab.doc2bow(doc) for doc in desc[year_count[i]:year_count[i+1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ae9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03943926",
   "metadata": {},
   "source": [
    "### Coherence and Perplexity\n",
    "\n",
    "We then wanted to find how many topics would be optimal for the LDA model. We thought we would try the range between 5 and 30 topics as we felt that it would provide enough topics to look at, while ensuring that analysis would be relatively easy. However after running the code for a while, it became obvious that coherence will strongly prefer few topics and so would not be a good metric for this. We used the mean to combine the coherence of the models for each year to allow us an easy way to compare the different sets of models using a simple metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e97b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence=-1000 #to initialise the coherence\n",
    "for topics in range(5,30):\n",
    "    ldamodels=[]\n",
    "    ldacoherence=[]\n",
    "    for i in range(23): #for each year create a model and test the coherence of that model on the data\n",
    "        ldamodels.append(LDA(corpus=doc_word_matrix_array[i], id2word=vocab, num_topics=topics)) \n",
    "        ldacoherence.append(CoherenceModel(model=ldamodels[i], corpus=doc_word_matrix_array[i], dictionary=vocab, coherence='u_mass').get_coherence())\n",
    "    temp_coherence = statistics.mean(ldacoherence)\n",
    "    #if the new model works better than previous ones then save its parameters\n",
    "    if temp_coherence > coherence:\n",
    "        coherence = temp_coherence\n",
    "        models = ldamodels\n",
    "    print(coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecca87e",
   "metadata": {},
   "source": [
    "We then tried to use perplexity instead. However, we came to a similar problem that perplexity strongly prefers models with a large number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55250a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity=1000 #to initialise the perplexity\n",
    "for topics in range(5,30):\n",
    "    ldamodels=[]\n",
    "    ldaperplexity=[]\n",
    "    for i in range(23): #for each year create a model and test the perplexity of that model on the data\n",
    "        ldamodels.append(LDA(corpus=doc_word_matrix_array[i], id2word=vocab, num_topics=topics))\n",
    "        ldaperplexity.append(ldamodels[i].log_perplexity(doc_word_matrix_array[i]))\n",
    "    temp_perplexity = statistics.mean(ldaperplexity)\n",
    "    #if the new model works better than previous ones then save its parameters\n",
    "    if temp_perplexity < perplexity:\n",
    "        perplexity = temp_perplexity\n",
    "        models = ldamodels\n",
    "    print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aeb9bd",
   "metadata": {},
   "source": [
    "### Jaccard Score\n",
    "\n",
    "The reason that we having issues with perplexity and coherence is because it doesn't take into consideration the number of words in each topic, which would give a balance to their scoring. Therefore, we thought that the Jaccard similarity would act as a good way of being able to balance this. \n",
    "\n",
    "In order calculate the jaccard score for the collection of topics we decided that we would sum the scores of each topic compared with every other topic. \n",
    "\n",
    "However, we found this did not work as by using a sum, it will increase as topics increase and so will not be a fair evaluation of the jaccard scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb063644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to extract the top 50 words in each topic and return a list of topics containing the list of 50 words\n",
    "def extract_Words(model,num_topics): \n",
    "    topics=[]\n",
    "    for words in model.show_topics(num_topics = num_topics, num_words = 50, formatted=False):\n",
    "        (a,b) = words #removes the index\n",
    "        word=[]\n",
    "        for j in b:\n",
    "            (c,d) = j #extracts the word from the (word, probability) tuple\n",
    "            word.append(c)\n",
    "        topics.append(word)\n",
    "    return topics\n",
    "\n",
    "#returns the 'jaccard score' of the set of topics\n",
    "def get_Jaccard(topics,num_topics):\n",
    "    score = 0\n",
    "    for i in range(len(topics)):\n",
    "        for j in range(len(topics)):\n",
    "            score = score + jaccard_score(topics[i],topics[j],average='macro')\n",
    "    return score\n",
    "\n",
    "jaccard=100\n",
    "for topics in range(5,30):\n",
    "    ldamodels=[]\n",
    "    ldajaccard=[]\n",
    "    for i in range(23):\n",
    "        ldamodels.append(LDA(corpus=doc_word_matrix_array[i], id2word=vocab, num_topics=topics))\n",
    "        ldajaccard.append(get_Jaccard(extract_Words(ldamodels[i],topics),topics))\n",
    "    temp_jaccard = statistics.mean(ldajaccard)\n",
    "    #if the new model works better than previous ones then save its parameters\n",
    "    if temp_jaccard < jaccard:\n",
    "        jaccard = temp_jaccard\n",
    "        models = ldamodels\n",
    "    print(jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ffbb04",
   "metadata": {},
   "source": [
    "After realising the sum did not work, we decided that multiplying would provide a better way of combining the jaccard scores of the topics. We also thought we would inverse the score as number close to 0 have more impact on the product so by inverting the score, we are highlighting when there is a large intersection between topics. We also accounted for the case where topics are equal by assigning any such values as non-0, while being much lower than any other possible value. \n",
    "\n",
    "However, after running this the model with 5 topics was optimal and we believe that this is much more likely due to the biased metric rather than 5 actually being the optimal number. In this case we believe that the metric is inhibited by the inability to be able to look at the whole set of words in each topic, and must instead choose a fixed value for each. This means that the number of words in each topic appears to be 50 for each, which is not the case and undermines the weighting that we hoped the jaccard score would provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e212d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7570220823758371\n",
      "0.7570220823758371\n",
      "0.7570220823758371\n",
      "0.7570220823758371\n",
      "0.7570220823758371\n",
      "0.7570220823758371\n",
      "0.7570220823758371\n",
      "0.7570220823758371\n",
      "0.7570220823758371\n",
      "0.7570220823758371\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4986/2846497430.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mldamodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_word_matrix_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mldajaccard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_inverse_Jaccard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_Words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtemp_jaccard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldajaccard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#if the new model works better than previous ones then save its parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4986/2846497430.py\u001b[0m in \u001b[0;36mget_inverse_Jaccard\u001b[0;34m(topics, num_topics)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to highlight when there is lots of intersection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0;31m#print(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mjaccard_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#used to extract the top 50 words in each topic and return a list of topics containing the list of 50 words\n",
    "def extract_Words(model,num_topics):\n",
    "    topics=[]\n",
    "    for words in model.show_topics(num_topics = num_topics, num_words = 50,formatted=False):\n",
    "        (a,b) = words #removes the index\n",
    "        word=[]\n",
    "        for j in b:\n",
    "            (c,d) = j #extracts the word from the (word, probability) tuple\n",
    "            word.append(c)\n",
    "        topics.append(word)\n",
    "    return topics\n",
    "\n",
    "#returns the 'jaccard score' of the set of topics\n",
    "def get_inverse_Jaccard(topics,num_topics):\n",
    "    score = 1\n",
    "    for i in range(len(topics)):\n",
    "        for j in range(len(topics)):\n",
    "            if i != j: #only compares different topics\n",
    "                s = 1 - jaccard_score(topics[i],topics[j],average='macro') # to highlight when there is lots of intersection\n",
    "                if s == 0:\n",
    "                    s = 0.0001\n",
    "                score = score * s\n",
    "    return score\n",
    "\n",
    "jaccard=0\n",
    "for topics in range(5,30):\n",
    "    ldamodels=[]\n",
    "    ldajaccard=[]\n",
    "    for i in range(23):\n",
    "        ldamodels.append(LDA(corpus=doc_word_matrix_array[i], id2word=vocab, num_topics=topics))\n",
    "        ldajaccard.append(get_inverse_Jaccard(extract_Words(ldamodels[i],topics),topics))\n",
    "    temp_jaccard = statistics.mean(ldajaccard)\n",
    "    #if the new model works better than previous ones then save its parameters\n",
    "    if temp_jaccard > jaccard:\n",
    "        jaccard = temp_jaccard\n",
    "        models = ldamodels\n",
    "    print(jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3df919",
   "metadata": {},
   "source": [
    "Here we could have created our own metric that would have looked at either combining a jaccard similarity with the probabilities associated with each word or combining perplexity / coherence with a gauge for the size of the topics. However, this was time constrained. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
