{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d428866",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Unfortunately, this project has been fraught with various dead-ends, which has resulted in very few analysable results. However, we do have some results that we are able to analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59587c7e",
   "metadata": {},
   "source": [
    "### The Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0781ce8",
   "metadata": {},
   "source": [
    "### The Issues\n",
    "\n",
    "#### Optimising Topic Numbers\n",
    "We wanted to see if we could find the optimal number of topics for our LDA model, as there was no clear way to perscribe one otherwise. However, after using coherence, perplexity and jaccard similarities, we found that each was baised on either having few or many topics. Therefore, we decided that we would use 50 as this was the number of topics used in the paper that gave us the idea for this project.\n",
    "\n",
    "#### The Sequence Model\n",
    "The project was based on the idea that we would be able to run an LDA sequence model that would allow us to easily see how the topics changed over time. However, we were unable to get the model to run efficiently and so had to disregard this idea. Having looked into the exact implementation of this model, the limiting factor is a function used for helping to calculate the posterior, and this function is limited in speed due to the size of the dictionary that we used. \n",
    " \n",
    "#### Topic Modelling on Independent Years\n",
    "After ending the sequence model route, we decided that we could try to model the years independently and then compare this to an LDA. The first implementation of this, was not able to handle the size of our data. So, we changed to a different Hungarian algorithm implementation. \n",
    "\n",
    "#### Independent Year topic merging\n",
    "After training the models on each year, we needed to correlate the different topics in each year. However, after looking at the jaccard score for this, we found that there was very little overlap between topics in different years. As such we decided to not include this for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0dcf04",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    " - One thing that we could have done that would have sped up the sequence model and may have provided better results is by removing words that only occur a couple of times. By removing infrequent words, we would have greatly reduced the size of the dictionary and many of those words are web addresses or similar, which are not particularly useful at finding the larger trends. This also, could have greatly improved the results gained from the indpendent years LDA because having lots of words that only appear in one year would have made it much harder to merge the topics between years.\n",
    " - If there was more time, it would have been nice to have created a similarity score that balanced the similarity of the topics with the number of topics modelled. On top of this, we could have created a score that accounted for the probabilies of the words being associated with each topic. Both of these might have been able to provide improved results from our models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
