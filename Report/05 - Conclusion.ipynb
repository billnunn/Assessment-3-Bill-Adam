{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d428866",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Unfortunately, this project has been fraught with various dead-ends, and our results are fewer and weaker than we'd hoped for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59587c7e",
   "metadata": {},
   "source": [
    "### The Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73b1fe",
   "metadata": {},
   "source": [
    "The main result from the analyses of Section 03 and 04 is that none of the specific methodologies we tried improved upon the performance of the baseline LDA. Indeed, our time based methods for extracting important words did not work. Even after we submitted defeat and hand picked some seeding words, the prior reweighting offered no improvement by UMass coherence score. In fact, the heavily weighted words didn't even end up in the original topics! In the final part of Section 03 we saw further evidence that prior seeding might not be a great idea: trying `gensim`'s the automatic prior reweighting reduced the topic models UMass coherence significantly. We think there may still be merit in using time based methods to extract important words (just not the method we tried!), and would have liked to work with a time which was less discrete. In Section 04 we saw the difficulty of trying to split the model to run on the individual years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0781ce8",
   "metadata": {},
   "source": [
    "### The Issues\n",
    "\n",
    "#### Optimising Topic Numbers\n",
    "We wanted to see if we could find the optimal number of topics for our LDA model, as there was no clear way to perscribe one otherwise. However, after using coherence, perplexity and jaccard similarities, we found that each was baised on either having few or many topics. Therefore, we decided that we would use 50 as this was the number of topics used in the paper that gave us the idea for this project.\n",
    "\n",
    "#### The Sequence Model\n",
    "The project was based on the idea that we would be able to run an LDA sequence model that would allow us to easily see how the topics changed over time. However, we were unable to get the model to run efficiently and so had to disregard this idea. Having looked into the exact implementation of this model, the limiting factor is a function used for helping to calculate the posterior, and this function is limited in speed due to the size of the dictionary that we used. \n",
    " \n",
    "#### Topic Modelling on Independent Years\n",
    "After ending the sequence model route, we decided that we could try to model the years independently and then compare this to an LDA. The first implementation of this, was not able to handle the size of our data. So, we changed to a different Hungarian algorithm implementation. \n",
    "\n",
    "#### Independent Year Topic Merging\n",
    "After training the models on each year, we needed to correlate the different topics in each year. However, after looking at the jaccard score for this, we found that there was very little overlap between topics in different years. As such we decided to not include this for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0dcf04",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    " - One thing that we could have done that would have sped up the sequence model and may have provided better results is by removing words that only occur a couple of times. By removing infrequent words, we would have greatly reduced the size of the dictionary and many of those words are web addresses or similar, which are not particularly useful at finding the larger trends. This also, could have greatly improved the results gained from the indpendent years LDA because having lots of words that only appear in one year would have made it much harder to merge the topics between years.\n",
    " - It may have been inappropriate to use time to derive topic titles given we only had access to CVE year. All of the analysis in Section 3 would have been more interesting, and I think more likely to work, if each document had a timestamp instead.\n",
    " - If there was more time, we would have carried out a much more extensive literature search of ways to derive word importance, and try some of these methods to find seeds for the prior. We got distracted by using time to extract word importance, when other, better tested, methods are available.\n",
    " - With even more time, it would have been nice to have created a similarity score that balanced the similarity of the topics with the number of topics modelled. On top of this, we could have created a score that accounted for the probabilies of the words being associated with each topic. Both of these might have been able to provide improved results from our models.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15050ec5",
   "metadata": {},
   "source": [
    "## References\n",
    "Original Idea - https://www.researchgate.net/publication/330625565_Analyzing_Evolving_Trends_of_Vulnerabilities_in_National_Vulnerability_Database\n",
    "Dynamic Topic Models - https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
